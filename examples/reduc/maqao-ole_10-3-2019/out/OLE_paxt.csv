Instance 0
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 1
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 2
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 3
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 4
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 5
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 6
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 7
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 8
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 9
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 10
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 11
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 12
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 13
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 14
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 15
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 16
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 17
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 18
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 19
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 20
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 21
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 22
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 23
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 24
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 25
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 26
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 27
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 28
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 29
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 30
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 31
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 32
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 33
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 34
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 35
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 36
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 37
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 38
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 39
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 40
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 41
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 42
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 43
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 44
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 45
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 46
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 47
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 48
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 49
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 50
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 51
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 52
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 53
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 54
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 55
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 56
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 57
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 58
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 59
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 60
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 61
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 62
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 63
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 64
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 65
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 66
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 67
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 68
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 69
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 70
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 71
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 72
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 73
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 74
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 75
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 76
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 77
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 78
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 79
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 80
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 81
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 82
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 83
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 84
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 85
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 86
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 87
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 88
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 89
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 90
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 91
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 92
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 93
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 94
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 95
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 96
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 97
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 98
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

Instance 99
0x3f81:	VMOVUPD (%RAX),%XMM15                             ;	linear pattern;	256
0x3f85:	VINSERTF128 $0x1,0x10(%RAX),%YMM15,%YMM3          ;	linear pattern;	256
0x3f96:	VMOVUPD -0xe0(%RAX),%XMM2                         ;	linear pattern;	256
0x3f9e:	VINSERTF128 $0x1,-0xd0(%RAX),%YMM2,%YMM7          ;	linear pattern;	256
0x3fa8:	VMOVUPD -0xc0(%RAX),%XMM6                         ;	linear pattern;	256
0x3fb0:	VINSERTF128 $0x1,-0xb0(%RAX),%YMM6,%YMM8          ;	linear pattern;	256
0x3fbe:	VMOVUPD -0xa0(%RAX),%XMM10                        ;	linear pattern;	256
0x3fc6:	VINSERTF128 $0x1,-0x90(%RAX),%YMM10,%YMM11        ;	linear pattern;	256
0x3fd0:	VMOVUPD -0x80(%RAX),%XMM13                        ;	linear pattern;	256
0x3fd5:	VINSERTF128 $0x1,-0x70(%RAX),%YMM13,%YMM14        ;	linear pattern;	256
0x3fdc:	VMOVUPD -0x60(%RAX),%XMM15                        ;	linear pattern;	256
0x3fe1:	VINSERTF128 $0x1,-0x50(%RAX),%YMM15,%YMM3         ;	linear pattern;	256
0x3fed:	VMOVUPD -0x40(%RAX),%XMM4                         ;	linear pattern;	256
0x3ff2:	VINSERTF128 $0x1,-0x30(%RAX),%YMM4,%YMM2          ;	linear pattern;	256
0x3ff9:	VMOVUPD -0x20(%RAX),%XMM6                         ;	linear pattern;	256
0x3ffe:	VINSERTF128 $0x1,-0x10(%RAX),%YMM6,%YMM8          ;	linear pattern;	256

